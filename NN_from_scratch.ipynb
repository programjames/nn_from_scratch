{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f02e31",
   "metadata": {},
   "source": [
    "# Neural Net Built From Scratch\n",
    "## By James Camacho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08498279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3942c",
   "metadata": {},
   "source": [
    "Activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cdedd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    def __init__(self, f, df):\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "    def __call__(self, x):\n",
    "        return self.f(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "    \n",
    "    def grad(self, x):\n",
    "        return self.df(x)\n",
    "        \n",
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsig(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0.1*x, x)\n",
    "\n",
    "def drelu(x):\n",
    "    return np.piecewise(x, [x <= 0, x > 0], [0.1, 1])\n",
    "\n",
    "Sigmoid = Activation(sig, dsig)\n",
    "ReLU = Activation(relu, drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1735a6b",
   "metadata": {},
   "source": [
    "Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f2e23884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def __init__(self, f, df):\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        return self.f(x, y)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return self.f(x, y)\n",
    "    \n",
    "    def grad(self, x, y):\n",
    "        return self.df(x, y)\n",
    "    \n",
    "def loss(output, y):\n",
    "    return np.sum((output-y)**2) / len(y)\n",
    "\n",
    "def dloss(output, y):\n",
    "    return 2*(output - y) / len(y)\n",
    "\n",
    "MSE = Loss(loss, dloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5595f4",
   "metadata": {},
   "source": [
    "Neural nets should have a bunch of layers. We're using sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "1b4ccec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, ins, outs, activation=ReLU):\n",
    "        self.w = rng.normal(1, 0.1, size=(ins, outs))\n",
    "        self.b = rng.normal(0, 0.1, size=outs)\n",
    "        self.a = activation\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.a(x @ self.w + self.b)\n",
    "    \n",
    "    def grad(self, x):\n",
    "        d = self.a.grad(x @ self.w + self.b)\n",
    "        return d\n",
    "    \n",
    "class NeuralNet(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, x, y, loss=MSE):\n",
    "        for layer in layers:\n",
    "            layer.db = layer.grad(x)\n",
    "            x = layer(x)\n",
    "        \n",
    "        dx = loss.grad(x, y)\n",
    "        for layer in reversed(layers):\n",
    "            layer.db *= dx\n",
    "            layer.dw = layer.w[None,] * layer.db[:,None]\n",
    "            dx = dx @ layer.w.T\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            layer.db = np.sum(layer.db, axis=0)\n",
    "            layer.dw = np.sum(layer.dw, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ab671",
   "metadata": {},
   "source": [
    "Optimizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "040174b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(ABC):\n",
    "    def __init__(self, nn):\n",
    "        self.nn = nn\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backprop(self):\n",
    "        pass\n",
    "    \n",
    "def penalize(nn, penalty=1e-3):\n",
    "    # penalize nn for large weights\n",
    "    for layer in nn.layers:\n",
    "        layer.db += layer.b * penalty\n",
    "        layer.dw += layer.w * penalty\n",
    "    \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, nn, alpha=1e-3, penalty=1e-3):\n",
    "        self.nn = nn\n",
    "        self.alpha = alpha\n",
    "        self.penalty = penalty\n",
    "    \n",
    "    def backprop(self):\n",
    "        penalize(nn, self.penalty)\n",
    "        for layer in self.nn.layers:\n",
    "            layer.b -= self.alpha * layer.db\n",
    "            layer.w -= self.alpha * layer.dw\n",
    "    \n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, nn, alpha=1e-3, beta=0.1, penalty=1e-3):\n",
    "        \"\"\"\n",
    "        nn - NeuralNet to optimize.\n",
    "        alpha - learning rate\n",
    "        beta - exponential decay rate (for signal/noise weighted mean)\n",
    "        \"\"\"\n",
    "        super().__init__(nn)\n",
    "        self.signals_b = None\n",
    "        self.noise_b = None\n",
    "        self.signals_w = None\n",
    "        self.noise_w = None\n",
    "        self.a = alpha\n",
    "        self.b = 1 - beta\n",
    "        self.bk = 1\n",
    "        self.penalty = penalty\n",
    "        \n",
    "    def reset(self):\n",
    "        self.signals_b = None\n",
    "        self.noise_b = None\n",
    "        self.signals_w = None\n",
    "        self.noise_w = None\n",
    "        self.bk = 1\n",
    "    \n",
    "    def backprop(self):\n",
    "        \"\"\"\n",
    "        Should call nn.train before this.\n",
    "        \"\"\"\n",
    "        # Add penalty for big values\n",
    "        penalize(nn, self.penalty)\n",
    "        \n",
    "        if self.signals_b is None:\n",
    "            self.signals_b = {}\n",
    "            self.noise_b = {}\n",
    "            self.signals_w = {}\n",
    "            self.noise_w = {}\n",
    "            for layer in self.nn.layers:\n",
    "                self.signals_b[layer] = layer.db\n",
    "                self.noise_b[layer] = layer.db ** 2\n",
    "                \n",
    "                self.signals_w[layer] = layer.dw\n",
    "                self.noise_w[layer] = layer.dw ** 2\n",
    "        \n",
    "        self.bk *= self.b\n",
    "        for layer in self.nn.layers:\n",
    "            self.signals_b[layer] = ((1-self.b) * layer.db + self.b * self.signals_b[layer] * (1-self.bk)) / (1-self.bk*self.b)\n",
    "            self.noise_b[layer] = ((1-self.b) * layer.db**2 + self.b * self.noise_b[layer] * (1-self.bk)) / (1-self.bk*self.b)\n",
    "            \n",
    "            self.signals_w[layer] = ((1-self.b) * layer.db + self.b * self.signals_w[layer] * (1-self.bk)) / (1-self.bk*self.b)\n",
    "            self.noise_w[layer] = ((1-self.b) * layer.db**2 + self.b * self.noise_w[layer] * (1-self.bk)) / (1-self.bk*self.b)\n",
    "            \n",
    "            db = self.a * self.signals_b[layer] / self.noise_b[layer]**0.5\n",
    "            dw = self.a * self.signals_w[layer] / self.noise_w[layer]**0.5\n",
    "            layer.b[np.isfinite(db)] -= db[np.isfinite(db)]\n",
    "            layer.w[np.isfinite(dw)] -= dw[np.isfinite(dw)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a9c49",
   "metadata": {},
   "source": [
    "We're going to have it learn the AND function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "8444d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = rng.integers(2, size=(100, 2))\n",
    "train_y = np.vstack(train_x[:, 0] & train_x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc89d20",
   "metadata": {},
   "source": [
    "Create our Neural Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "0a5c01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Layer(2, 100), Layer(100, 1)]\n",
    "nn = NeuralNet(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84380cc6",
   "metadata": {},
   "source": [
    "Optimize. Intially start with SGD, then finish with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "b3283d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c772ba8218b3435eb1f6a60fb9caf00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim = SGD(nn, alpha=1e-4)\n",
    "epochs = 1000\n",
    "pbar = tqdm(range(epochs))\n",
    "for i in pbar:\n",
    "    nn.train(train_x, train_y)\n",
    "    optim.backprop()\n",
    "    if i % 100 == 0:\n",
    "        pred = nn(train_x)\n",
    "        loss = MSE(pred, train_y)\n",
    "        pbar.set_description(\"Loss: %.3f\" % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "f59c4ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dcd1ea95cd459eb985b50207d0d708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim = Adam(nn, alpha=1e-4)\n",
    "epochs = 10000\n",
    "pbar = tqdm(range(epochs))\n",
    "for i in pbar:\n",
    "    nn.train(train_x, train_y)\n",
    "    optim.backprop()\n",
    "    if i % 100 == 0:\n",
    "        pred = nn(train_x)\n",
    "        loss = MSE(pred, train_y)\n",
    "        pbar.set_description(\"Loss: %.3f\" % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "e0c74841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0 is -0.15923\n",
      "0 AND 1 is -0.01149\n",
      "1 AND 0 is -0.01174\n",
      "1 AND 1 is 1.02833\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 1]:\n",
    "    for j in [0, 1]:\n",
    "        print(f\"{i} AND {j} is {nn([i,j])[0]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e65979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
